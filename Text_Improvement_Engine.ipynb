{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04a85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861a73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim.downloader as api\n",
    "#path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09764368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model in the word2vec text format\n",
    "word2vec_model_path = \"GoogleNews-vectors-negative300.bin\"  # Path to the Word2Vec model file\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b8c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of phrases\n",
    "std_phrases = [\"Optimal performance\", \n",
    "           \"Utilise resources\",\n",
    "            \"Enhance productivity\", \n",
    "           \"Conduct an analysis\", \n",
    "           \"Maintain a high standard\", \n",
    "           \"Implement best practices\", \n",
    "           \"Ensure compliance\", \n",
    "           \"Streamline operations\", \n",
    "           \"Foster innovation\", \n",
    "           \"Drive growth\", \n",
    "           \"Leverage synergies\", \n",
    "           \"Demonstrate leadership\", \n",
    "           \"Exercise due diligence\", \n",
    "           \"Maximize stakeholder value\", \n",
    "           \"Prioritise tasks\", \n",
    "           \"Facilitate collaboration\", \n",
    "           \"Monitor performance metrics\", \n",
    "           \"Execute strategies\", \n",
    "           \"Gauge effectiveness\", \n",
    "           \"Champion change\"]\n",
    "\n",
    "\n",
    "# Input text\n",
    "input_text = \"In today's meeting, we discussed a variety of issues affecting our department. The weather was unusually sunny, a pleasant backdrop to our serious discussions. We came to the consensus that we need to do better in terms of performance. Sally brought doughnuts, which lightened the mood. It's important to make good use of what we have at our disposal. During the coffee break, we talked about the upcoming company picnic. We should aim to be more efficient and look for ways to be more creative in our daily tasks. Growth is essential for our future, but equally important is building strong relationships with our team members. As a reminder, the annual staff survey is due next Friday. Lastly, we agreed that we must take time to look over our plans carefully and consider all angles before moving forward. On a side note, David mentioned that his cat is recovering well from surgery.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a384d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text preprocessing, deletion of punctuation marks, stopwords\n",
    "def cleanText(text):\n",
    "    text_nopunct = re.sub(r'[^\\w\\s]','',text.lower())\n",
    "    word_tokens=nltk.word_tokenize(text_nopunct)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c447f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting into the phrase vectors (embeddings)\n",
    "def phrase_vector(words):\n",
    "    valid_words = [word for word in words if word in word2vec_model]\n",
    "    if valid_words:\n",
    "        sentence_vector = sum(word2vec_model[word] for word in valid_words) / len(valid_words)\n",
    "        return sentence_vector\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df14c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for generating phrases (2-5 words)\n",
    "def phrase_generator(sentence):\n",
    "    words=cleanText(sentence)\n",
    "    phrases=[]\n",
    "    for length in range(2, min(6, len(words) + 1)):  \n",
    "        for i in range(len(words) - length + 1):\n",
    "            phrase = \" \".join(words[i:i+length])\n",
    "            phrases.append(phrase)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996c0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for calculation cosine similarity between 2 vectors\n",
    "def calculate_similarity(phrase,std_phrase):\n",
    "    embd_std_phrases=phrase_vector(cleanText(std_phrase))\n",
    "    embd_phrase=phrase_vector(cleanText(phrase))\n",
    "    similarity_score = cosine_similarity([embd_std_phrases], [embd_phrase])[0][0]\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167d4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display original text with stopwords \n",
    "def display_phrases(phrase,input_text):\n",
    "    result_string = \" \"\n",
    "    word_tokens=nltk.word_tokenize(phrase)\n",
    "    pattern = re.compile(rf\"{word_tokens[0]}(.*?){word_tokens[-1]}\", re.DOTALL)\n",
    "    matches = re.findall(pattern, input_text.lower())\n",
    "    result_string = f\"{word_tokens[0]} {' '.join(matches)} {word_tokens[-1]}\"\n",
    "    return result_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145f5704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Phrases\tRecomended Substitude Phrases\tCosine Similarity Score\n",
      "--------------------------------------------------------------------------------\n",
      "1\tBetter  in terms of  performance\t\tOptimal performance\t\t0.64\n",
      "2\tNeed  to do better in terms of  performance\t\tOptimal performance\t\t0.61\n",
      "3\tDaily   tasks\t\tPrioritise tasks\t\t0.83\n",
      "4\tCreative  in our daily  tasks\t\tPrioritise tasks\t\t0.74\n",
      "5\tWays  to be more creative in our daily  tasks\t\tPrioritise tasks\t\t0.70\n",
      "6\tLook  for ways to be more creative in our daily  tasks\t\tPrioritise tasks\t\t0.66\n",
      "7\tGrowth  is  essential\t\tDrive growth\t\t0.67\n",
      "8\tGrowth  is essential for our  future\t\tDrive growth\t\t0.61\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(input_text)\n",
    "print(\"Original Phrases\\tRecomended Substitude Phrases\\tCosine Similarity Score\")\n",
    "print(\"-\" * 80)\n",
    "cnt=0\n",
    "for sentence in sentences:\n",
    "    phrases_list=phrase_generator(sentence)\n",
    "    for std_phrase in std_phrases:        \n",
    "        for phrase in phrases_list:  \n",
    "            similarity_score =calculate_similarity(phrase,std_phrase)\n",
    "            if similarity_score>0.6 and similarity_score<1:\n",
    "                cnt += 1\n",
    "                print(f\"{cnt}\\t{display_phrases(phrase,input_text).capitalize()}\\t\\t{std_phrase.capitalize()}\\t\\t{similarity_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
